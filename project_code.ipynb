{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiadgelis/ML-AudioEnhancement-Project/blob/main/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_sfVDxxs-W7"
      },
      "source": [
        "---   \n",
        "# **Audio Restoration for Generative Models — Improving MusicGen Outputs**\n",
        "---  \n",
        "\n",
        "**L’obiettivo** di questo progetto, apparentemente semplice, è quello di migliorare un audio generato da MusicGen. In realtà, la sfida principale consiste nell’aumentarne la qualità sia dal punto di vista percettivo sia da quello tecnico, mantenendo inalterata l’accuratezza semantica rispetto al prompt originario.\n",
        "\n",
        "Nella **prima cella** del notebook è stata predisposta l’inizializzazione dell’ambiente di lavoro mediante l’installazione e l’importazione delle librerie necessarie, e tramite la creazione delle cartelle per il salvataggio degli output del progetto:   \n",
        "* **torch** e **torchaudio**: la prima costituisce uno strumento fondamentale per il Machine Learning e il Deep Learning, in quanto consente la gestione di tensori e modelli neurali; la seconda è dedicata al processamento e alla manipolazione di segnali audio.  \n",
        "\n",
        "* **numpy**: utilizzata per il calcolo scientifico, in particolare per la gestione di array e operazioni numeriche.\n",
        "\n",
        "* **librosa**: fondamentale per analizzare e processare i file audio, la useremo durante l'analisi finale.\n",
        "\n",
        "* **librosa.display** e **matplotlib**: visualizzano i risultati delle precedenti analisi (spettrogrammi, waveform) creando grafici e diagrammi.\n",
        "\n",
        "* **os** e **glob**: consentono la gestione e la navigazione di file e directory.\n",
        "\n",
        "* **transformers**: libreria essenziale per il caricamento e l’utilizzo di modelli pre-addestrati, tra cui MusicGen che noi andiamo ad utilizzare.\n",
        "\n",
        "* **scipy.io.wavfile (write)**: impiegata per la scrittura di file audio in formato .wav.\n",
        "\n",
        "* **IPython.display (Audio, display)**: permette la riproduzione e la visualizzazione dei file audio direttamente all’interno di notebook interattivi come Jupyter.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uC7ukhd_s95Z"
      },
      "outputs": [],
      "source": [
        "# installazione librerie, tra cui demucs che andremo a utilizzare per il miglioramento audio\n",
        "!pip install torch torchaudio numpy librosa scipy soundfile matplotlib transformers accelerate demucs\n",
        "\n",
        "# import delle librerie\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# creazione delle cartelle di lavoro\n",
        "\n",
        "os.makedirs('musicgen_output', exist_ok=True) # cartella per audio generato\n",
        "os.makedirs('processed_audio', exist_ok=True) # cartella per audio processato\n",
        "os.makedirs('demucs_output', exist_ok=True) # cartella per gli output di Demucs\n",
        "os.makedirs('enhanced_audio', exist_ok=True) # cartella per audio finale\n",
        "\n",
        "print(\"Setup completato. Librerie importate e cartelle create.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyUfb84-K_3y"
      },
      "source": [
        "Nella **seconda cella** viene effettuato il caricamento del modello **MusicGen**, sviluppato da Meta AI, un modello di text-to-music generation che è in grado di generare sequenze audio coerenti a partire da un prompt in linguaggio naturale, mantenendo sia la struttura musicale che lo stile richiesto.\n",
        "\n",
        "In questo progetto, il modello viene inizializzato per leggere il prompt ( **prompt_text**) fornito e generare l’audio corrispondente. Una volta prodotto, il file audio viene salvato all’interno della cartella predefinita.\n",
        "\n",
        "Volendo nella riga 11, per rendere più dinamico il progetto, si può inserire una funzione:\n",
        "```\n",
        "prompt_text = [input('inserisci prompt personalizzato':)]\n",
        "```\n",
        "che permette all'utente di definire l'audio che vuole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duvBrEWVtkMy"
      },
      "outputs": [],
      "source": [
        "# caricamento MusicGen\n",
        "\n",
        "print(\"Caricamento del modello MusicGen...\")\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"Modello MusicGen caricato su: {device}\")\n",
        "\n",
        "# definizione del prompt\n",
        "prompt_text = [\"An 80s pop song with heavy synth, a groovy bassline, and punchy drums\"]\n",
        "\n",
        "# generazione dell'audio\n",
        "print(\"generazione dell'audio in corso...\")\n",
        "inputs = processor(\n",
        "    text=prompt_text,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# per una durata maggiore aumentiamo i token\n",
        "audio_values = model.generate(**inputs, max_new_tokens=768)\n",
        "sampling_rate = model.config.audio_encoder.sampling_rate\n",
        "print(\"Generazione completata.\")\n",
        "\n",
        "# salvataggio dell'output\n",
        "# normalizziamo e convertiamo in 16-bit PCM per simulare un output standard\n",
        "audio_numpy = audio_values.cpu().numpy().squeeze()\n",
        "audio_numpy = np.int16(audio_numpy / np.max(np.abs(audio_numpy)) * 32767)\n",
        "\n",
        "musicgen_raw_path = 'musicgen_output/raw_output_32k_16bit.wav'\n",
        "write_wav(musicgen_raw_path, sampling_rate, audio_numpy)\n",
        "\n",
        "print(f\"File audio salvato in: {musicgen_raw_path}\")\n",
        "print(\"Output di MusicGen:\")\n",
        "display(Audio(musicgen_raw_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNRKlNzVOQJP"
      },
      "source": [
        "Nella **terza cella** viene eseguita la fase di **pre-processing** del segnale audio, accompagnata dalla conversione di formato mediante operazioni di **Digital Signal Processing (DSP)**. Questa fase ha lo scopo di predisporre il suono agli step successivi di elaborazione: le modifiche effettuate non sono percepibili all’ascolto, ma sono necessarie dal punto di vista tecnico per garantire la compatibilità e la qualità del segnale.\n",
        "\n",
        "La funzione principale implementa tre operazioni:\n",
        "\n",
        "* Caricamento dei file audio: i dati vengono letti dal disco e trasferiti in memoria per poter essere elaborati.\n",
        "\n",
        "* Upsampling: la frequenza di campionamento viene incrementata da 32 kHz a 48 kHz, migliorando la risoluzione temporale del segnale e rendendolo conforme a standard audio più diffusi.\n",
        "\n",
        "* Conversione di formato: i dati audio vengono trasformati dal formato a interi a 16 bit in numeri in virgola mobile a 32 bit (32-bit float), garantendo una maggiore precisione numerica. Successivamente, i file vengono nuovamente salvati su disco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCPZ0hJKvzCQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_audio_stage1_alternative(input_path, output_path, target_sr=48000):\n",
        "\n",
        "    print(f\"Inizio pre-processing per: {input_path} \")\n",
        "\n",
        "    waveform, original_sr = torchaudio.load(input_path)\n",
        "    print(f\"File caricato. SR originale: {original_sr} Hz, Dtype: {waveform.dtype}\")\n",
        "\n",
        "    # uso del resampler interno di torchaudio\n",
        "    resampler = torchaudio.transforms.Resample(\n",
        "        orig_freq=original_sr,\n",
        "        new_freq=target_sr,\n",
        "        resampling_method=\"sinc_interpolation\"\n",
        "    )\n",
        "    resampled_waveform = resampler(waveform)\n",
        "    print(f\"Upsampling a {target_sr} Hz completato.\")\n",
        "\n",
        "    # salvataggio in formato 32-bit float\n",
        "    torchaudio.save(\n",
        "        output_path,\n",
        "        resampled_waveform,\n",
        "        target_sr,\n",
        "        encoding=\"PCM_F\",\n",
        "        bits_per_sample=32\n",
        "    )\n",
        "    print(f\"--- File processato salvato in: {output_path} ---\")\n",
        "\n",
        "processed_path = 'processed_audio/processed_48k_32bit.wav'\n",
        "# chiamata della nuova funzione\n",
        "preprocess_audio_stage1_alternative(musicgen_raw_path, processed_path)\n",
        "\n",
        "print(\"\\nAudio dopo il Pre-Processing (48kHz, 32-bit float):\")\n",
        "display(Audio(processed_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAU6PcreQ9lg"
      },
      "source": [
        "Nella **quarta cella** viene eseguita la fase finale del progetto e tratta proprio il miglioramento del segnale audio attraverso **Demucs (Deep Extractor for Music Sources)**, un modello di source separation sviluppato da Meta AI e che permette di scomporre un brano nelle sue componenti fondamentali con un’elevata fedeltà.\n",
        "\n",
        "La procedura adottata nel progetto segue due fasi principali:\n",
        "\n",
        "* Separazione: il modello analizza il brano e lo scompone nelle sue tracce costitutive, tipicamente basso, batteria, voce e una traccia “altro” che include gli strumenti rimanenti.\n",
        "\n",
        "* Ricombinazione: le tracce separate vengono successivamente riunite per ricostruire l’audio completo.\n",
        "\n",
        "Il principio alla base di questo approccio è che, durante il processo di separazione e ricostruzione, il modello tende a eliminare i disturbi, i rumori di fondo e gli artefatti che non appartengono a nessuna delle componenti strumentali principali. Il risultato finale è un audio tecnicamente più pulito, definito e percettivamente superiore rispetto all’originale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz8Usidr3o_X"
      },
      "outputs": [],
      "source": [
        "import shutil # fornisce la possibilità di supportare la copia e la rimozione\n",
        "\n",
        "print(\" miglioramento audio con Demucs\")\n",
        "\n",
        "demucs_output_dir = \"demucs_output\"\n",
        "\n",
        "# esecuzione del modello\n",
        "print(\"Esecuzione di Demucs in corso... (separazione completa)\")\n",
        "!demucs \"{processed_path}\" -o \"{demucs_output_dir}\"\n",
        "\n",
        "print(\"\\n--- Elaborazione Demucs completata ---\")\n",
        "\n",
        "# file di output (bass, drums, other, vocals) che vanno ricercati\n",
        "try:\n",
        "    # costruzione del percorso di ricerca\n",
        "    base_name = os.path.basename(processed_path).replace('.wav', '')\n",
        "    # modello di default è 'htdemucs'\n",
        "    search_path = os.path.join(demucs_output_dir, \"htdemucs\", base_name, \"*.wav\")\n",
        "\n",
        "    output_stems = glob.glob(search_path)\n",
        "\n",
        "    if len(output_stems) < 1: # può genera meno di 4 stem se sono vuoti\n",
        "        raise FileNotFoundError(\"Nessuna traccia separata trovata. Controlla l'output di Demucs.\")\n",
        "\n",
        "    print(f\"Trovate {len(output_stems)} tracce separate:\")\n",
        "    for stem_path in output_stems:\n",
        "        print(f\" - {os.path.basename(stem_path)}\")\n",
        "\n",
        "    # cariacmento e unione tracce\n",
        "    combined_waveform = None\n",
        "    final_sr = None\n",
        "\n",
        "    for stem_path in output_stems:\n",
        "        waveform, sr = torchaudio.load(stem_path)\n",
        "        if combined_waveform is None:\n",
        "            combined_waveform = waveform\n",
        "            final_sr = sr\n",
        "        else:\n",
        "            # controllo che i tensori abbiano la stessa lunghezza\n",
        "            target_length = min(combined_waveform.shape[1], waveform.shape[1])\n",
        "            combined_waveform = combined_waveform[:, :target_length] + waveform[:, :target_length]\n",
        "\n",
        "    # salvataggio file finale\n",
        "    enhanced_path = os.path.join('enhanced_audio', 'final_enhanced_output_recombined.wav')\n",
        "    torchaudio.save(enhanced_path, combined_waveform, final_sr)\n",
        "    print(f\"\\nTracce ricombinate e salvate in: {enhanced_path}\")\n",
        "\n",
        "    # audio finale\n",
        "    print(\"\\nAudio dopo il Miglioramento (Finale con Demucs Ricombinato):\")\n",
        "    display(Audio(enhanced_path))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERRORE: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x59YUq9T44v"
      },
      "source": [
        "# **Analisi degli output**\n",
        "---\n",
        "Nelle **ultime celle**  del notebook l’attenzione viene posta sull’analisi dei risultati ottenuti, attraverso strumenti sia visivi che quantitativi. In particolare, vengono utilizzati:\n",
        "\n",
        "* lo spettrogramma, che consente una rappresentazione visiva del segnale sonoro e delle sue variazioni nel tempo;\n",
        "\n",
        "* un grafico a barre che mostra, in termini numerici e percentuali, il miglioramento della qualità audio.\n",
        "\n",
        "**Quinta cella: calcolo e visualizzazione degli spettrogrammi**  \n",
        "\n",
        "In questa fase viene calcolato lo spettrogramma per tre diverse versioni del segnale: l’audio originale, quello processato e quello finale.\n",
        "Lo spettrogramma è una rappresentazione bidimensionale che mostra le frequenze (da quelle più basse a quelle più alte) presenti in un suono e la loro evoluzione temporale. I grafici vengono visualizzati uno sotto l’altro, permettendo così un confronto diretto della “trama” del segnale acustico prima e dopo il miglioramento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlztiChn8gj8"
      },
      "outputs": [],
      "source": [
        "# la prima funzione serve per il plot dello spettrogramma\n",
        "def plot_spectrogram(filepath, title, ax):\n",
        "    y, sr = librosa.load(filepath)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    img = librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
        "    ax.set_title(title)\n",
        "    return img\n",
        "\n",
        "# creazione grafici\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
        "\n",
        "# 1. Spettrogramma Originale\n",
        "plot_spectrogram(musicgen_raw_path, f'1. Originale MusicGen ({torchaudio.info(musicgen_raw_path).sample_rate} Hz)', axs[0])\n",
        "\n",
        "# 2. Spettrogramma dopo Pre-Processing (Upsampling)\n",
        "plot_spectrogram(processed_path, f'2. Dopo Upsampling ({torchaudio.info(processed_path).sample_rate} Hz)', axs[1])\n",
        "\n",
        "# 3. Spettrogramma dopo Miglioramento\n",
        "img = plot_spectrogram(enhanced_path, f'3. Finale Migliorato ({torchaudio.info(enhanced_path).sample_rate} Hz)', axs[2])\n",
        "\n",
        "fig.colorbar(img, ax=axs, format='%+2.0f dB', label='Intensità')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ascolto audio finale\n",
        "print(\"--- Riepilogo Audio ---\")\n",
        "print(\"\\n1. Originale (da MusicGen):\")\n",
        "display(Audio(musicgen_raw_path))\n",
        "\n",
        "print(\"\\n2. Dopo Upsampling a 48kHz (Stage 1):\")\n",
        "display(Audio(processed_path))\n",
        "\n",
        "print(\"\\n3. Finale (dopo Miglioramento Neurale):\")\n",
        "display(Audio(enhanced_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2cHjmT3CxGu"
      },
      "source": [
        "**Sesta cella: calcolo delle metriche e rappresentazione grafica**\n",
        "\n",
        "Nell’ultima cella si procede con la valutazione quantitativa del miglioramento attraverso il calcolo di metriche specifiche sia per l’audio originale sia per quello finale:\n",
        "\n",
        "* Spectral Centroid: indice che misura la “brillantezza” del suono, legato alla concentrazione delle frequenze più alte.\n",
        "\n",
        "* Spectral Flatness: indice che valuta il grado di “rumorosità” di un segnale; valori più bassi corrispondono a un suono più tonale e meno rumoroso.\n",
        "\n",
        "* RMS Energy: misura del livello energetico medio, interpretabile come il volume percepito del segnale.\n",
        "\n",
        "Infine, i risultati vengono rappresentati tramite un grafico a barre che evidenzia la variazione percentuale di ciascun indice, rendendo immediatamente chiaro l’impatto del processo di miglioramento audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob8ko2dECZzC"
      },
      "outputs": [],
      "source": [
        "def calculate_and_compare_metrics(original_path, enhanced_path):\n",
        "    print(\"Analisi Comparativa della Qualità Audio\")\n",
        "\n",
        "    # caricamento file\n",
        "    y_orig, sr_orig = librosa.load(original_path)\n",
        "    y_enh, sr_enh = librosa.load(enhanced_path)\n",
        "\n",
        "    # calcolo metriche\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # per l'audio originale\n",
        "    metrics['original'] = {\n",
        "        'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y_orig, sr=sr_orig)),\n",
        "        'spectral_flatness': np.mean(librosa.feature.spectral_flatness(y=y_orig)),\n",
        "        'rms_energy': np.mean(librosa.feature.rms(y=y_orig))\n",
        "    }\n",
        "\n",
        "    # per l'audio migliorato\n",
        "    metrics['enhanced'] = {\n",
        "        'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y_enh, sr=sr_enh)),\n",
        "        'spectral_flatness': np.mean(librosa.feature.spectral_flatness(y=y_enh)),\n",
        "        'rms_energy': np.mean(librosa.feature.rms(y=y_enh))\n",
        "    }\n",
        "\n",
        "    # risultati\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" RISULTATI NUMERICI\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{'Metrica':<20} | {'Originale':<15} | {'Migliorato':<15}\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "    for key in metrics['original']:\n",
        "        orig_val = metrics['original'][key]\n",
        "        enh_val = metrics['enhanced'][key]\n",
        "        print(f\"{key:<20} | {orig_val:<15.4f} | {enh_val:<15.4f}\")\n",
        "    print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# creazione grafico di confronto\n",
        "# valori dell'originale posti pari al 100%\n",
        "def plot_comparison(metrics):\n",
        "    labels = list(metrics['original'].keys())\n",
        "    original_values = np.array(list(metrics['original'].values()))\n",
        "    enhanced_values = np.array(list(metrics['enhanced'].values()))\n",
        "\n",
        "    # normalizzazione dei valori\n",
        "    enhanced_normalized = (enhanced_values / original_values) * 100\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    rects1 = ax.bar(x - width/2, [100] * len(labels), width, label='Originale (Riferimento 100%)', color='skyblue')\n",
        "    rects2 = ax.bar(x + width/2, enhanced_normalized, width, label='Migliorato (vs Originale)', color='salmon')\n",
        "\n",
        "    ax.set_ylabel('Valore Normalizzato (%)')\n",
        "    ax.set_title('Confronto Qualità Audio: Originale vs. Migliorato')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels, rotation=15)\n",
        "    ax.axhline(100, color='grey', linestyle='--', linewidth=0.8)\n",
        "    ax.legend()\n",
        "\n",
        "    ax.bar_label(rects2, fmt='%.1f%%', padding=3)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# esecuzione analisi\n",
        "\n",
        "try:\n",
        "    comparison_metrics = calculate_and_compare_metrics(musicgen_raw_path, enhanced_path)\n",
        "    plot_comparison(comparison_metrics)\n",
        "except NameError:\n",
        "    print(\"ERRORE: sono eseguite le celle precedenti? sono le variabili 'musicgen_raw_path' e 'enhanced_path' definite?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}